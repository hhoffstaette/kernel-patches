From 65abee380a9ced613440a468df828b899cea88b3 Mon Sep 17 00:00:00 2001
From: Neal Cardwell <ncardwell@google.com>
Date: Fri, 30 Mar 2018 10:30:43 -0700
Subject: [PATCH 5/6] net-tcp_bbr: drain_to_target: drain inflight to BDP once per cycle

In BBR v1.0 the "drain" phase of the pacing gain cycle holds the
pacing_gain to 0.75 for essentially 1*min_rtt (or less if inflight
falls below the BDP).

This commit modifies the behavior of this "drain" phase to attempt to
"drain to target", adaptively holding this "drain" phase until
inflight reaches the target level that matches the estimated BDP
(bandwidth-delay product).

This can significantly reduce the amount of data queued at the
bottleneck, and hence reduce queuing delay and packet loss, in cases
where there are multiple flows sharing a bottleneck.

Ingredients to make this work:

+ Bounded time in drain phase: We need to refresh our bandwidth
estimate periodically, before "good" bandwidth samples age out of the
fillter. So this mechanism makes sure to restart the cycle with the
bandwidth-probing phase of the cycle at least once per CYCLE_LEN=8 *
min_rtt.

+ Randomized phases: We need to avoid elephants and mice probing and
draining in sync. Otherwise any mice that are probing in sync with an
elephant will repeatedly be out-competed for bandwidth, and its
estimated bandwidth will be driven progressively lower. In this
commit, each cycle is now of random length: 2 to 8 x min_rtt.

+ Aggregation estimator: To adaptively allow higher cwnd with
aggregation effects, this mechanism needs to be deployed with a
companion mechanism that can allow higher cwnds when this is required
due to aggregation on the path.

Signed-off-by: Neal Cardwell <ncardwell@google.com>
Reviewed-by: Yuchung Cheng <ycheng@google.com>
Reviewed-by: Priyaranjan Jha <priyarjha@google.com>
---
 net/ipv4/tcp_bbr.c | 78 +++++++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 77 insertions(+), 1 deletion(-)

diff --git a/net/ipv4/tcp_bbr.c b/net/ipv4/tcp_bbr.c
index ba4f34934b80..dc18432d4cc2 100644
--- a/net/ipv4/tcp_bbr.c
+++ b/net/ipv4/tcp_bbr.c
@@ -97,9 +97,10 @@ struct bbr {
 		packet_conservation:1,  /* use packet conservation? */
 		restore_cwnd:1,	     /* decided to revert cwnd to old value */
 		round_start:1,	     /* start of packet-timed tx->ack round? */
+		cycle_len:4,	     /* phases in this PROBE_BW gain cycle */
 		idle_restart:1,	     /* restarting after idle? */
 		probe_rtt_round_done:1,  /* a BBR_PROBE_RTT round at 4 pkts? */
-		unused:12,
+		unused:8,
 		lt_is_sampling:1,    /* taking long-term ("LT") samples now? */
 		lt_rtt_cnt:7,	     /* round trips in long-term interval */
 		lt_use_bw:1;	     /* use lt_bw as our bw estimate? */
@@ -197,6 +198,9 @@ static const u32 bbr_ack_epoch_acked_reset_thresh = 1U << 20;
 /* Time period for clamping cwnd increment due to ack aggregation */
 static const u32 bbr_extra_acked_max_us = 100 * 1000;
 
+/* Each cycle, try to hold sub-unity gain until inflight <= BDP. */
+static const bool bbr_drain_to_target = true;	/* default: enabled */
+
 /* Do we estimate that STARTUP filled the pipe? */
 static bool bbr_full_bw_reached(const struct sock *sk)
 {
@@ -547,6 +551,72 @@ static bool bbr_is_next_cycle_phase(struct sock *sk,
 		inflight <= bbr_inflight(sk, bw, BBR_UNIT);
 }
 
+static void bbr_set_cycle_idx(struct sock *sk, int cycle_idx)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr->cycle_idx = cycle_idx;
+	bbr->pacing_gain = bbr->lt_use_bw ?
+			   BBR_UNIT : bbr_pacing_gain[bbr->cycle_idx];
+}
+
+static void bbr_drain_to_target_cycling(struct sock *sk,
+					const struct rate_sample *rs)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 elapsed_us =
+		tcp_stamp_us_delta(tp->delivered_mstamp, bbr->cycle_mstamp);
+	u32 inflight, bw;
+
+	if (bbr->mode != BBR_PROBE_BW)
+		return;
+
+	/* Always need to probe for bw before we forget good bw estimate. */
+	if (elapsed_us > bbr->cycle_len * bbr->min_rtt_us) {
+		/* Start a new PROBE_BW probing cycle of [2 to 8] x min_rtt. */
+		bbr->cycle_mstamp = tp->delivered_mstamp;
+		bbr->cycle_len = CYCLE_LEN - prandom_u32_max(bbr_cycle_rand);
+		bbr_set_cycle_idx(sk, BBR_BW_PROBE_UP);  /* probe bandwidth */
+		return;
+	}
+
+	/* The pacing_gain of 1.0 paces at the estimated bw to try to fully
+	 * use the pipe without increasing the queue.
+	 */
+	if (bbr->pacing_gain == BBR_UNIT)
+		return;
+
+	inflight = rs->prior_in_flight;  /* what was in-flight before ACK? */
+	bw = bbr_max_bw(sk);
+
+	/* A pacing_gain < 1.0 tries to drain extra queue we added if bw
+	 * probing didn't find more bw. If inflight falls to match BDP then we
+	 * estimate queue is drained; persisting would underutilize the pipe.
+	 */
+	if (bbr->pacing_gain < BBR_UNIT) {
+		if (inflight <= bbr_inflight(sk, bw, BBR_UNIT))
+			bbr_set_cycle_idx(sk, BBR_BW_PROBE_CRUISE); /* cruise */
+		return;
+	}
+
+	/* A pacing_gain > 1.0 probes for bw by trying to raise inflight to at
+	 * least pacing_gain*BDP; this may take more than min_rtt if min_rtt is
+	 * small (e.g. on a LAN). We do not persist if packets are lost, since
+	 * a path with small buffers may not hold that much. Similarly we exit
+	 * if we were prevented by app/recv-win from reaching the target.
+	 */
+	if (elapsed_us > bbr->min_rtt_us &&
+	    (inflight >= bbr_inflight(sk, bw, bbr->pacing_gain) ||
+	     rs->losses ||         /* perhaps pacing_gain*BDP won't fit */
+	     rs->is_app_limited || /* previously app-limited */
+	     !tcp_send_head(sk) || /* currently app/rwin-limited */
+	     !tcp_snd_wnd_test(tp, tcp_send_head(sk), tp->mss_cache))) {
+		bbr_set_cycle_idx(sk, BBR_BW_PROBE_DOWN);  /* drain queue */
+		return;
+	}
+}
+
 static void bbr_advance_cycle_phase(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -564,6 +634,11 @@ static void bbr_update_cycle_phase(struct sock *sk,
 {
 	struct bbr *bbr = inet_csk_ca(sk);
 
+	if (bbr_drain_to_target) {
+		bbr_drain_to_target_cycling(sk, rs);
+		return;
+	}
+
 	if (bbr->mode == BBR_PROBE_BW && bbr_is_next_cycle_phase(sk, rs))
 		bbr_advance_cycle_phase(sk);
 }
@@ -994,6 +1069,7 @@ static void bbr_init(struct sock *sk)
 	bbr->full_bw_cnt = 0;
 	bbr->cycle_mstamp = 0;
 	bbr->cycle_idx = 0;
+	bbr->cycle_len = 0;
 	bbr_reset_lt_bw_sampling(sk);
 	bbr_reset_startup_mode(sk);
 
-- 
2.17.0.484.g0c8726318c-goog

