From a2722d9b2422eaa58d36b9d1cc589a70d2117ceb Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 11 Nov 2014 22:58:12 +0800
Subject: [PATCH 03/22] bfs: [Sync] resched_curr()

Complete replace old resched_task() with new resched_curr() as mainline
3.17 does.
---
 kernel/sched/bfs.c | 28 ++++++++++++----------------
 1 file changed, 12 insertions(+), 16 deletions(-)

diff --git a/kernel/sched/bfs.c b/kernel/sched/bfs.c
index 3244afd..2164cc4 100644
--- a/kernel/sched/bfs.c
+++ b/kernel/sched/bfs.c
@@ -600,12 +600,7 @@ static inline int task_timeslice(struct task_struct *p)
 	return (rr_interval * task_prio_ratio(p) / 128);
 }
 
-static void resched_task(struct task_struct *p);
-
-static inline void resched_curr(struct rq *rq)
-{
-	resched_task(rq->curr);
-}
+static void resched_curr(struct rq *rq);
 
 /*
  * qnr is the "queued but not running" count which is the total number of
@@ -1114,24 +1109,25 @@ static inline void __set_tsk_resched(struct task_struct *p)
 }
 
 /*
- * resched_task - mark a task 'to be rescheduled now'.
+ * resched_curr - mark rq's current task 'to be rescheduled now'.
  *
  * On UP this means the setting of the need_resched flag, on SMP it
  * might also involve a cross-CPU call to trigger the scheduler on
  * the target CPU.
  */
-void resched_task(struct task_struct *p)
+void resched_curr(struct rq *rq)
 {
+	struct task_struct *curr = rq->curr;
 	int cpu;
 
 	lockdep_assert_held(&grq.lock);
 
-	if (test_tsk_need_resched(p))
+	if (test_tsk_need_resched(curr))
 		return;
 
-	set_tsk_need_resched(p);
+	set_tsk_need_resched(curr);
 
-	cpu = task_cpu(p);
+	cpu = cpu_of(rq);
 	if (cpu == smp_processor_id()) {
 		set_preempt_need_resched();
 		return;
@@ -3719,7 +3715,7 @@ void rt_mutex_setprio(struct task_struct *p, int prio)
 		dequeue_task(p);
 	p->prio = prio;
 	if (task_running(p) && prio > oldprio)
-		resched_task(p);
+		resched_curr(rq);
 	if (queued) {
 		enqueue_task(p, rq);
 		try_preempt(p, rq);
@@ -3780,7 +3776,7 @@ void set_user_nice(struct task_struct *p, long nice)
 	} else if (task_running(p)) {
 		reset_rq_task(rq, p);
 		if (old_static < new_static)
-			resched_task(p);
+			resched_curr(rq);
 	}
 out_unlock:
 	task_grq_unlock(&flags);
@@ -3914,7 +3910,7 @@ __setscheduler(struct task_struct *p, struct rq *rq, int policy, int prio)
 		reset_rq_task(rq, p);
 		/* Resched only if we might now be preempted */
 		if (p->prio > oldprio || p->rt_priority > oldrtprio)
-			resched_task(p);
+			resched_curr(rq);
 	}
 }
 
@@ -5084,7 +5080,7 @@ void resched_cpu(int cpu)
 	unsigned long flags;
 
 	grq_lock_irqsave(&flags);
-	resched_task(cpu_curr(cpu));
+	resched_curr(cpu_rq(cpu));
 	grq_unlock_irqrestore(&flags);
 }
 
@@ -5233,7 +5229,7 @@ int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
 			set_tsk_need_resched(p);
 			running_wrong = true;
 		} else
-			resched_task(p);
+			resched_curr(rq);
 	} else
 		set_task_cpu(p, cpumask_any_and(cpu_active_mask, new_mask));
 
-- 
2.4.6

