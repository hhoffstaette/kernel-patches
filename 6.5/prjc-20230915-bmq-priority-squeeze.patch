
https://gitlab.com/alfredchen/linux-prjc/-/issues/82

diff --git a/include/linux/sched/prio.h b/include/linux/sched/prio.h
index 6af9ae681116..a9a1dfa99140 100644
--- a/include/linux/sched/prio.h
+++ b/include/linux/sched/prio.h
@@ -26,7 +26,7 @@
 
 /* +/- priority levels from the base priority */
 #ifdef CONFIG_SCHED_BMQ
-#define MAX_PRIORITY_ADJ	(7)
+#define MAX_PRIORITY_ADJ	(12)
 
 #define MIN_NORMAL_PRIO		(MAX_RT_PRIO)
 #define MAX_PRIO		(MIN_NORMAL_PRIO + NICE_WIDTH)
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 3e8ddbd8001c..21ea7c351b66 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -622,7 +622,7 @@ static inline void update_rq_clock(struct rq *rq)
 	if (unlikely(delta <= 0))
 		return;
 	rq->clock += delta;
-	update_rq_time_edge(rq);
+	sched_update_rq_clock(rq);
 	update_rq_clock_task(rq, delta);
 }
 
@@ -2971,6 +2971,8 @@ static int try_to_wake_up(struct task_struct *p, unsigned int state,
 		set_task_cpu(p, cpu);
 	}
 #else
+	sched_task_ttwu(p);
+
 	cpu = task_cpu(p);
 #endif /* CONFIG_SMP */
 
@@ -4127,7 +4129,6 @@ void scheduler_tick(void)
 
 	task_tick_mm_cid(rq, rq->curr);
 
-	rq->last_tick = rq->clock;
 	raw_spin_unlock(&rq->lock);
 
 	if (sched_feat(LATENCY_WARN) && resched_latency)
@@ -4616,6 +4617,16 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 }
 #endif
 
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	p->time_slice = sched_timeslice_ns;
+
+	sched_task_renew(p, rq);
+
+	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
+		requeue_task(p, rq, task_sched_prio_idx(p, rq));
+}
+
 /*
  * Timeslices below RESCHED_NS are considered as good as expired as there's no
  * point rescheduling when there's so little time left.
@@ -4824,8 +4835,10 @@ static void __sched notrace __schedule(unsigned int sched_mode)
 #endif
 
 	if (likely(prev != next)) {
-		next->last_ran = rq->clock_task;
+#ifdef CONFIG_SCHED_BMQ
 		rq->last_ts_switch = rq->clock;
+#endif
+		next->last_ran = rq->clock_task;
 
 		/*printk(KERN_INFO "sched: %px -> %px\n", prev, next);*/
 		rq->nr_switches++;
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 5494f27cdb04..869d8b6461fc 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -14,16 +14,16 @@
 
 #include "cpupri.h"
 
-#ifdef CONFIG_SCHED_BMQ
-/* bits:
- * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
-#define SCHED_LEVELS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
-#endif
-
-#ifdef CONFIG_SCHED_PDS
-/* bits: RT(0-24), reserved(25-31), SCHED_NORMAL_PRIO_NUM(32), cpu idle task(1) */
-#define SCHED_LEVELS	(64 + 1)
-#endif /* CONFIG_SCHED_PDS */
+#define MIN_SCHED_NORMAL_PRIO	(32)
+/*
+ * levels: RT(0-24), reserved(25-31), NORMAL(32-63), cpu idle task(64)
+ *
+ * -- BMQ --
+ * NORMAL: (lower boost range 12, NICE_WIDTH 40, higher boost range 12) / 2
+ * -- PDS --
+ * NORMAL: SCHED_EDGE_DELTA + ((NICE_WIDTH 40) / 2)
+ */
+#define SCHED_LEVELS		(64 + 1)
 
 #define IDLE_TASK_SCHED_PRIO	(SCHED_LEVELS - 1)
 
@@ -199,9 +199,11 @@ struct rq {
 	unsigned long calc_load_update;
 	long calc_load_active;
 
-	u64 clock, last_tick;
-	u64 last_ts_switch;
+	u64 clock;
 	u64 clock_task;
+#ifdef CONFIG_SCHED_BMQ
+	u64 last_ts_switch;
+#endif
 
 	unsigned int  nr_running;
 	unsigned long nr_uninterruptible;
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index f29b8f3aa786..d8f6381c27a9 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -4,8 +4,7 @@
  * BMQ only routines
  */
 #define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
-#define boost_threshold(p)	(sched_timeslice_ns >>\
-				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
+#define boost_threshold(p)	(sched_timeslice_ns >> ((14 - (p)->boost_prio) / 2))
 
 static inline void boost_task(struct task_struct *p)
 {
@@ -46,7 +45,8 @@ task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 
 static inline int task_sched_prio(const struct task_struct *p)
 {
-	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
+	return (p->prio < MAX_RT_PRIO)? (p->prio >> 2) :
+		MIN_SCHED_NORMAL_PRIO + (p->prio + p->boost_prio - MAX_RT_PRIO) / 2;
 }
 
 static inline int
@@ -65,24 +65,15 @@ static inline int sched_idx2prio(int idx, struct rq *rq)
 	return idx;
 }
 
-static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
-{
-	p->time_slice = sched_timeslice_ns;
-
-	if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
-		if (SCHED_RR != p->policy)
-			deboost_task(p);
-		requeue_task(p, rq, task_sched_prio_idx(p, rq));
-	}
-}
-
-static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq) {}
-
 inline int task_running_nice(struct task_struct *p)
 {
 	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
 }
 
+static inline void sched_update_rq_clock(struct rq *rq) {}
+static inline void sched_task_renew(struct task_struct *p, const struct rq *rq) {}
+static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq) {}
+
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
 	p->boost_prio = MAX_PRIORITY_ADJ;
@@ -93,18 +84,18 @@ static inline void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
 	p->boost_prio = MAX_PRIORITY_ADJ;
 }
 
-#ifdef CONFIG_SMP
 static inline void sched_task_ttwu(struct task_struct *p)
 {
 	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
 		boost_task(p);
 }
-#endif
 
 static inline void sched_task_deactivate(struct task_struct *p, struct rq *rq)
 {
-	if (rq_switch_time(rq) < boost_threshold(p))
+	u64 switch_ns = rq_switch_time(rq);
+
+	if (switch_ns < boost_threshold(p))
 		boost_task(p);
+	else if (switch_ns > sched_timeslice_ns)
+		deboost_task(p);
 }
-
-static inline void update_rq_time_edge(struct rq *rq) {}
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 15cc4887efed..b20226ed47cc 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -1,6 +1,5 @@
 #define ALT_SCHED_NAME "PDS"
 
-#define MIN_SCHED_NORMAL_PRIO	(32)
 static const u64 RT_MASK = ((1ULL << MIN_SCHED_NORMAL_PRIO) - 1);
 
 #define SCHED_NORMAL_PRIO_NUM	(32)
@@ -68,18 +67,12 @@ static inline int sched_idx2prio(int sched_idx, struct rq *rq)
 		MIN_SCHED_NORMAL_PRIO + SCHED_NORMAL_PRIO_MOD(sched_idx - rq->time_edge);
 }
 
-static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
-{
-	if (p->prio >= MIN_NORMAL_PRIO)
-		p->deadline = rq->time_edge + (p->static_prio - (MAX_PRIO - NICE_WIDTH)) / 2;
-}
-
 int task_running_nice(struct task_struct *p)
 {
 	return (p->prio > DEFAULT_PRIO);
 }
 
-static inline void update_rq_time_edge(struct rq *rq)
+static inline void sched_update_rq_clock(struct rq *rq)
 {
 	struct list_head head;
 	u64 old = rq->time_edge;
@@ -94,7 +87,6 @@ static inline void update_rq_time_edge(struct rq *rq)
 	delta = min_t(u64, SCHED_NORMAL_PRIO_NUM, now - old);
 	INIT_LIST_HEAD(&head);
 
-	/*printk(KERN_INFO "sched: update_rq_time_edge 0x%016lx %llu\n", rq->queue.bitmap[0], delta);*/
 	prio = MIN_SCHED_NORMAL_PRIO;
 	for_each_set_bit_from(prio, rq->queue.bitmap, MIN_SCHED_NORMAL_PRIO + delta)
 		list_splice_tail_init(rq->queue.heads + MIN_SCHED_NORMAL_PRIO +
@@ -121,12 +113,10 @@ static inline void update_rq_time_edge(struct rq *rq)
 		MIN_SCHED_NORMAL_PRIO : rq->prio - delta;
 }
 
-static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+static inline void sched_task_renew(struct task_struct *p, const struct rq *rq)
 {
-	p->time_slice = sched_timeslice_ns;
-	sched_renew_deadline(p, rq);
-	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
-		requeue_task(p, rq, task_sched_prio_idx(p, rq));
+	if (p->prio >= MIN_NORMAL_PRIO)
+		p->deadline = rq->time_edge + (p->static_prio - (MAX_PRIO - NICE_WIDTH)) / 2;
 }
 
 static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
@@ -138,15 +128,15 @@ static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
 
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
-	sched_renew_deadline(p, rq);
+	sched_task_renew(p, rq);
 }
 
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq);
+
 static inline void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
 {
 	time_slice_expired(p, rq);
 }
 
-#ifdef CONFIG_SMP
 static inline void sched_task_ttwu(struct task_struct *p) {}
-#endif
 static inline void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
